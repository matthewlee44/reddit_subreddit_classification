{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db93648e",
   "metadata": {},
   "source": [
    "# Reddit Subreddit Classification\n",
    "## Notebook 1 - Data Acquisition and Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6acfa6",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Subreddit posts related to dating and relationships seem to cover many common issues and situations. In many of these posts, users lament the frustrations and struggles of finding a romantic partner among a sea of people with vastly different familial, financial and other personal interests.\n",
    "\n",
    "This project aims to build a classifier using natural language processing that can distinguish between posts from the subreddit r/dating and r/datingoverforty as accurately as possible despite their similarities.\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Although many of the discussions within both of these subreddits share common themes and issues, I have determined that natural language processing tools can uncover enough differences to create an accurate classifer of their posts.\n",
    "\n",
    "### Findings\n",
    "\n",
    "While exploring the data, I ran a logistic regression model which calculated coefficients on each word used in the posts of these subreddits. These coefficients provide a gauge for the level of impact the presence of a word in a post has in the likelihood that post is predicted to be in one subreddit or the other. The words/topics with the most predictive impact in the logistic regression model for each subreddit is summarized below.\n",
    "\n",
    "| **r/dating**                | **r/datingoverforty**        |\n",
    "|-----------------------------|------------------------------|\n",
    "| Numbers in the 20s          | Numbers in the 40s and above |\n",
    "| Girl, girlfriend, boyfriend | Divorce                      |\n",
    "| School/College              | Children                     |\n",
    "|                             | Marriage                     |\n",
    "\n",
    "### Results\n",
    "\n",
    "I ran the text data through 5 different models with various pre-processing strategies and model hyper-parameters. The best model was a logistic regression model implementing a function that preprocessed the text using the insights from the analysis above. This model achieved an accuracy score of 82.02%. A full summary of the metrics achieved by model are provided below.\n",
    "\n",
    "| **Metric**      | **Score** |\n",
    "|-----------------|-----------|\n",
    "| Accuracy Score  | 82.02%    |\n",
    "| Precision Score | 82%       |\n",
    "| Recall Score    | 82%       |\n",
    "| F1 Score        | 82%       |\n",
    "\n",
    "To further improve upon the accuracy of this model, I would like to explore taking the following next steps: (i) adding more features about the text such as (a) sentiment analysis on each post and (b) counts of the number of words and sentences used in each post, and (ii) exploring other natural language processing strategies to differentiate the subreddits further.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a50c9b-9c72-48a4-b812-85dae722e204",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "Data for the two selected subreddits was acquired from the Pushshift API using the following code. Submissions were collected starting from 1 year prior to March 24th, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e98c18",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a0b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fd2b8",
   "metadata": {},
   "source": [
    "### Data Acquisition Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea51c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base url for Pushshift API\n",
    "base_url = \"https://api.pushshift.io/reddit/search/submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81285237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make request from Pushshift API and return data in dataframe\n",
    "# sleep code inspired by: https://realpython.com/python-sleep/\n",
    "\n",
    "def extract_reddit_data(subreddit, size, after):\n",
    "    # Create URL for request from pushshift based on subreddit, size of data response and starting time period\n",
    "    url = base_url + f\"?subreddit={subreddit}&size={size}&after={after}&sort=asc\"\n",
    "    \n",
    "    # Set variable to use in while loop which will keep trying if our request doens't get a 200 status code response\n",
    "    no_response = True\n",
    "    \n",
    "    # While loop to make requests as long as response status code is not 200\n",
    "    while no_response:\n",
    "        print(f'Making request at: {url}')\n",
    "        res = requests.get(url)\n",
    "        print(f'Request response code: {res.status_code}')\n",
    "        if res.status_code == 200:\n",
    "            no_response = False\n",
    "        else:\n",
    "            print(f'Trying again')\n",
    "            time.sleep(3)\n",
    "            \n",
    "    # Saving response data into dictionary\n",
    "    data = res.json()['data']\n",
    "    \n",
    "    # Print length of data to check response was not empty\n",
    "    print(f'Length of Data: {len(data)}')\n",
    "    \n",
    "    # Loop through response to collect certain items for each post into a list\n",
    "    submissions = []\n",
    "    for submission in data:\n",
    "        s = {\n",
    "            \"id\": submission.get('id', \"\"),\n",
    "            \"created_utc\": submission.get('created_utc',\"\"),\n",
    "            \"title\": submission.get(\"title\", \"\"),\n",
    "            \"selftext\": submission.get(\"selftext\", \"\"),\n",
    "            \"subreddit\": submission.get(\"subreddit\", \"\"),\n",
    "            \"subreddit_id\": submission.get(\"subreddit_id\", \"\"),\n",
    "            \"url\": submission.get(\"url\", \"\"),\n",
    "        }\n",
    "        submissions.append(submission)\n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8eb5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to making all requests necessary to collect a year's worth of posts for a subreddit\n",
    "\n",
    "def extract_all_submissions(subreddit, after):\n",
    "    # Get first set of submissions\n",
    "    print(f'Getting first set of submissions')\n",
    "    first = extract_reddit_data(subreddit, \"100\", after)\n",
    "    \n",
    "    # Write first set of submissions to CSV\n",
    "    pd.DataFrame(first).to_csv(f\"../data/{subreddit}-full-{after}.csv\")\n",
    "    \n",
    "    # Get timestamp of last post in first set to use for the next API call\n",
    "    last_timestamp = datetime.datetime.fromtimestamp(first[-1]['created_utc'])\n",
    "\n",
    "    # While loop to keep making API calls as long as the timestamp of the last post collected is before today's date\n",
    "    # Writes each API response to CSV\n",
    "    while last_timestamp.date() < datetime.date.today():\n",
    "        time.sleep(3)\n",
    "        print(f'Getting submissions after {last_timestamp}')\n",
    "        next_submissions = extract_reddit_data(subreddit, \"100\", str(int(last_timestamp.timestamp())))\n",
    "        pd.DataFrame(next_submissions).to_csv(f\"../data/{subreddit}-full-{last_timestamp}.csv\")\n",
    "        last_timestamp = datetime.datetime.fromtimestamp(next_submissions[-1]['created_utc'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc55bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquiring data from first subreddit; commented out to avoid running it again\n",
    "\n",
    "# extract_all_submissions(\"dating\", \"365d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0778f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquiring data from second subreddit; commented out to avoid running it again\n",
    "\n",
    "#extract_all_submissions(\"datingoverforty\", \"365d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e440f-f492-480e-a705-81ff6b19b1ef",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "After all the data obtained from the Pushshift API was written to CSV, the CSV files were then each read into a pandas DataFrame and concatenated. Then, the data was processed with some light cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3b3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect all csvs for a subreddit and return a dataframe\n",
    "\n",
    "def subreddit_to_df(subreddit):\n",
    "\n",
    "    # Get file names in data folder for subreddit\n",
    "    subreddit_files = []\n",
    "    for file in os.listdir(\"../data\"):\n",
    "        split_filename = file.split(\"-\")\n",
    "        if split_filename[0] == subreddit and split_filename[1] != \"full\":\n",
    "            subreddit_files.append(file)\n",
    "        \n",
    "    # Read csvs into dataframes then concatenate each of them \n",
    "    df = pd.concat([pd.read_csv(f'../data/{file}') for file in subreddit_files]).drop(columns=\"Unnamed: 0\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc6d7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process dataframe by removing columns, dealing with missing text\n",
    "\n",
    "def process_df(df):\n",
    "    # Create new copy of dataframe to be processed\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # Replace all \"[removed]\" values in dataframe with np.nan\n",
    "    new_df.replace(\"[removed]\", np.nan, inplace=True)\n",
    "    \n",
    "    # Drop np.nans\n",
    "    new_df.dropna(inplace=True)\n",
    "    \n",
    "    # Create new column with text from the title and the body of the post\n",
    "    new_df['alltext'] = new_df['title'].str.cat(new_df['selftext'], sep = \" \\n \")\n",
    "    \n",
    "    # Drop all columns except for alltext and subreddit which will become our target\n",
    "    new_df = new_df[[\"alltext\", \"subreddit\"]]\n",
    "    \n",
    "    # Set new target column by setting it to 0 where subreddit is dating, and 1 where datingoverforty\n",
    "    new_df.loc[:, \"||__target__||\"] = np.where(new_df[\"subreddit\"] == \"datingoverforty\", 1, 0)\n",
    "    \n",
    "    # Return dataframe with just alltext and new target column\n",
    "    return new_df[[\"alltext\", \"||__target__||\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeef2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes of subreddit posts\n",
    "dating_df = subreddit_to_df(\"dating\")\n",
    "over_forty_df = subreddit_to_df(\"datingoverforty\")\n",
    "\n",
    "# Process dataframes\n",
    "dating_df = process_df(dating_df)\n",
    "over_forty_df = process_df(over_forty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7182c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "model_df = pd.concat([dating_df, over_forty_df])\n",
    "\n",
    "# Reindex dataframe after the series of concatenations\n",
    "model_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c82b12-39b4-4605-b0a6-99871d5bccf5",
   "metadata": {},
   "source": [
    "### Fixing imbalanced classes in data\n",
    "Our final dataset is very imbalanced due to posts being submitted more frequently for one subreddit over the other. The following code was used to balance the classes before writing the final dataset to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dca6707-4d45-4e60-b84c-471e339c9d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.927439\n",
       "1    0.072561\n",
       "Name: ||__target__||, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show imbalanced classes\n",
    "model_df['||__target__||'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96126247-bff5-4400-b46d-d5e863f0169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create under sampled dataframe for the majority class matching the number of samples in the minority class\n",
    "under_sample_0 = model_df[model_df[\"||__target__||\"]==0].sample(n=4584, random_state=42, replace=False)\n",
    "\n",
    "# Concatenate undersampled dataframe with minority class data frame\n",
    "model_df = pd.concat([under_sample_0, model_df[model_df[\"||__target__||\"]==1]])\n",
    "\n",
    "# Reset index after sampling and concatenation\n",
    "model_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e62cd-0824-4121-9c29-d679a8f165b8",
   "metadata": {},
   "source": [
    "## Addition of Sentiment Analysis\n",
    "After the primary set of data comprising text and the target variables was composed, sentiment analysis from NLTK's Vader sentiment analyzer was added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f244bb23-3840-4c80-9cec-709ccc6e485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Vader sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create list of sentiment analysis results\n",
    "sentiments = []\n",
    "\n",
    "for text in list(model_df['alltext']):\n",
    "    sentiments.append(sia.polarity_scores(text))\n",
    "\n",
    "# Create dataframe from list of sentiments\n",
    "sentiments_df = pd.DataFrame(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e61ebdd-751e-4f94-bab4-2b226f5638ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment results to main dataframe\n",
    "model_df = pd.concat([model_df, sentiments_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51319145-fbf9-4726-b25c-6b2276176f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns so that they don't interfere with any columns that may be made later by text vectorizers\n",
    "model_df.rename(columns={\n",
    "    \"neg\": \"||__neg__||\",\n",
    "    \"neu\": \"||__neu__||\",\n",
    "    \"pos\": \"||__pos__||\",\n",
    "    \"compound\": \"||__compound__||\",\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03234cab-b947-41b8-a893-58bae331e748",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce925b0-a066-4fac-9310-97a919248923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final dataset to CSV\n",
    "model_df.to_csv(\"../data/final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
